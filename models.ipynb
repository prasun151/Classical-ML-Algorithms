{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/prasun/impli/Iris.csv')\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "y = pd.get_dummies(df['Species']).values  # One-hot encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.445368</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.750000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
       "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
       "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
       "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
       "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
       "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
       "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
       "max    150.000000       7.900000      4.400000       6.900000      2.500000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 2 1 0 2 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 1 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def fit(self, X, y):\n",
    "        # Add bias term and solve using normal equation\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(self.theta)\n",
    "\n",
    "# Train model\n",
    "model = SimpleLinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and get class labels\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=1000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.theta = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for _ in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self._sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return self._sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "# Train model using LogisticRegression\n",
    "logistic_models = []\n",
    "y_pred_prob = np.zeros((X_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "# Train one model per class (One-vs-All)\n",
    "for i in range(y_train.shape[1]):\n",
    "    model = LogisticRegression(lr=0.1, num_iter=1000)\n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    logistic_models.append(model)\n",
    "    y_pred_prob[:, i] = model.predict_proba(X_test)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class NewtonLogisticRegression:\n",
    "    def __init__(self, num_iter=10, tol=1e-4):\n",
    "        self.num_iter = num_iter\n",
    "        self.tol = tol\n",
    "        self.theta = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add bias term\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.zeros(n_features)\n",
    "        \n",
    "        for _ in range(self.num_iter):\n",
    "            # Compute sigmoid predictions\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self._sigmoid(z)\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = np.dot(X.T, (h - y)) / n_samples\n",
    "            \n",
    "            # Compute Hessian\n",
    "            W = np.diag(h * (1 - h))\n",
    "            hessian = (X.T @ W @ X) / n_samples\n",
    "            \n",
    "            # Newton step\n",
    "            try:\n",
    "                delta = np.linalg.solve(hessian, gradient)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Add small constant to diagonal if hessian is singular\n",
    "                delta = np.linalg.solve(hessian + 1e-5 * np.eye(n_features), gradient)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.theta -= delta\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.sum(np.abs(delta)) < self.tol:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return self._sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "# Train model using Newton's method\n",
    "logistic_models = []\n",
    "y_pred_prob = np.zeros((X_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "# Train one model per class (One-vs-All)\n",
    "for i in range(y_train.shape[1]):\n",
    "    model = NewtonLogisticRegression(num_iter=10)  # Fewer iterations needed for Newton's method\n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    logistic_models.append(model)\n",
    "    y_pred_prob[:, i] = model.predict_proba(X_test)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 1 2 2 2 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "KNN Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            # Calculate distances to all training points\n",
    "            distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "            # Get k nearest neighbor indices\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            # Get their labels\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            # Predict majority class\n",
    "            most_common = np.bincount(k_nearest_labels).argmax()\n",
    "            predictions.append(most_common)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = X_scaled[:-30]\n",
    "X_test_scaled = X_scaled[-30:]\n",
    "\n",
    "# Convert one-hot encoded y to class labels\n",
    "y_train_labels = np.argmax(y[:-30], axis=1)\n",
    "y_test_true = np.argmax(y[-30:], axis=1)\n",
    "\n",
    "# Train KNN model\n",
    "model = KNN(k=3)\n",
    "model.fit(X_train_scaled, y_train_labels)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_true)\n",
    "print(f\"KNN Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Random Forest Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=None, max_features='sqrt'):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        \n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        if self.max_features == 'sqrt':\n",
    "            self.feature_subset_size = int(np.sqrt(n_features))\n",
    "        else:\n",
    "            self.feature_subset_size = n_features\n",
    "            \n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            feature_idxs = np.random.choice(n_features, self.feature_subset_size, replace=False)\n",
    "            tree.fit(X_sample[:, feature_idxs], y_sample)\n",
    "            self.trees.append((feature_idxs, tree))\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X[:, feature_idxs]) \n",
    "                              for feature_idxs, tree in self.trees])\n",
    "        return np.array([np.bincount(pred_row).argmax() \n",
    "                        for pred_row in predictions.T])\n",
    "\n",
    "# Scale the data and prepare labels\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Split the scaled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForest(n_trees=100, max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decsion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "        \n",
    "    def _split(self, X, y, feature, threshold):\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        return (X[left_mask], y[left_mask], X[~left_mask], y[~left_mask])\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                gain = self._gini(y) - (len(y_left) * self._gini(y_left) + \n",
    "                                      len(y_right) * self._gini(y_right)) / len(y)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1:\n",
    "            self.tree = {'prediction': np.bincount(y).argmax()}\n",
    "            return self\n",
    "            \n",
    "        feature, threshold = self._best_split(X, y)\n",
    "        \n",
    "        if feature is None:\n",
    "            self.tree = {'prediction': np.bincount(y).argmax()}\n",
    "            return self\n",
    "            \n",
    "        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
    "        \n",
    "        left_tree = DecisionTree(self.max_depth)\n",
    "        right_tree = DecisionTree(self.max_depth)\n",
    "        \n",
    "        left_tree.fit(X_left, y_left, depth + 1)\n",
    "        right_tree.fit(X_right, y_right, depth + 1)\n",
    "        \n",
    "        self.tree = {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        tree = self.tree\n",
    "        while 'prediction' not in tree:\n",
    "            if x[tree['feature']] <= tree['threshold']:\n",
    "                tree = tree['left'].tree\n",
    "            else:\n",
    "                tree = tree['right'].tree\n",
    "        return tree['prediction']\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x) for x in X])\n",
    "\n",
    "# Scale the data and prepare labels\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Split the scaled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree model\n",
    "model = DecisionTree(max_depth=3)  # Reduced depth for better visualization\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Decision Tree Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Naive Bayes Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.vars = None\n",
    "        \n",
    "    def _gaussian_pdf(self, x, mean, var):\n",
    "        return np.exp(-(x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Calculate class priors\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.vars = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.priors[i] = len(X_c) / n_samples\n",
    "            self.means[i] = X_c.mean(axis=0)\n",
    "            self.vars[i] = X_c.var(axis=0) + 1e-9  # Add small value for numerical stability\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes)\n",
    "        probs = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            prior = np.log(self.priors[i])\n",
    "            likelihood = np.sum(np.log(self._gaussian_pdf(X, self.means[i], self.vars[i])), axis=1)\n",
    "            probs[:, i] = prior + likelihood\n",
    "            \n",
    "        # Convert log probabilities to probabilities\n",
    "        probs = np.exp(probs - probs.max(axis=1)[:, np.newaxis])\n",
    "        return probs / probs.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.classes[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "# Scale the data and prepare labels\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Split the scaled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 2 2 0 2 2 2 2 2 0 0 0 0 2 2 2 2 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "LDA Accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "class LinearDiscriminantAnalysis:\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.classes = None\n",
    "        self.means = None\n",
    "        self.weights = None\n",
    "        self.covariance = None\n",
    "        self.priors = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Calculate class means and priors\n",
    "        self.means = []\n",
    "        self.priors = []\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            X_c = X[y == cls]\n",
    "            self.means.append(X_c.mean(axis=0))\n",
    "            self.priors.append(len(X_c) / n_samples)\n",
    "        \n",
    "        self.means = np.array(self.means)\n",
    "        self.priors = np.array(self.priors)\n",
    "        \n",
    "        # Calculate within-class covariance matrix\n",
    "        self.covariance = np.zeros((n_features, n_features))\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            X_c = X[y == cls]\n",
    "            X_c = X_c - self.means[i]\n",
    "            self.covariance += X_c.T @ X_c\n",
    "        \n",
    "        self.covariance /= n_samples\n",
    "        \n",
    "        # Calculate weights using inverse covariance\n",
    "        try:\n",
    "            inv_covariance = np.linalg.inv(self.covariance)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Add small identity matrix if singular\n",
    "            inv_covariance = np.linalg.inv(self.covariance + 1e-4 * np.eye(n_features))\n",
    "            \n",
    "        self.weights = self.means @ inv_covariance\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        discriminant = X @ self.weights.T\n",
    "        discriminant += np.log(self.priors) - 0.5 * np.sum(self.means @ self.weights.T, axis=1)\n",
    "        \n",
    "        # Convert to probabilities using softmax\n",
    "        exp_disc = np.exp(discriminant - discriminant.max(axis=1)[:, np.newaxis])\n",
    "        probs = exp_disc / exp_disc.sum(axis=1)[:, np.newaxis]\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.classes[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "# Scale the data and prepare labels\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Split the scaled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LDA model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"LDA Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Gradient Boosting Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = {}\n",
    "        \n",
    "    def _mse_split(self, X, y):\n",
    "        best_score = float('inf')\n",
    "        best_split = None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                if left_mask.sum() == 0 or left_mask.sum() == len(y):\n",
    "                    continue\n",
    "                    \n",
    "                left_pred = y[left_mask].mean()\n",
    "                right_pred = y[~left_mask].mean()\n",
    "                score = np.sum((y[left_mask] - left_pred)**2) + np.sum((y[~left_mask] - right_pred)**2)\n",
    "                \n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_split = (feature, threshold, left_pred, right_pred)\n",
    "                    \n",
    "        return best_split\n",
    "    \n",
    "    def fit(self, X, y, depth=0):\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1:\n",
    "            self.tree = {\"prediction\": y.mean()}\n",
    "            return\n",
    "            \n",
    "        split = self._mse_split(X, y)\n",
    "        if split is None:\n",
    "            self.tree = {\"prediction\": y.mean()}\n",
    "            return\n",
    "            \n",
    "        feature, threshold, left_pred, right_pred = split\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        \n",
    "        self.tree = {\n",
    "            \"feature\": feature,\n",
    "            \"threshold\": threshold,\n",
    "            \"left\": DecisionTreeRegressor(self.max_depth),\n",
    "            \"right\": DecisionTreeRegressor(self.max_depth)\n",
    "        }\n",
    "        \n",
    "        self.tree[\"left\"].fit(X[left_mask], y[left_mask], depth + 1)\n",
    "        self.tree[\"right\"].fit(X[~left_mask], y[~left_mask], depth + 1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        node = self.tree\n",
    "        while \"prediction\" not in node:\n",
    "            if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "                node = node[\"left\"].tree\n",
    "            else:\n",
    "                node = node[\"right\"].tree\n",
    "        return node[\"prediction\"]\n",
    "\n",
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def _softmax(self, x):\n",
    "        exp_x = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        y_encoded = np.eye(n_classes)[y]\n",
    "        \n",
    "        # Initialize predictions with zeros\n",
    "        F = np.zeros((X.shape[0], n_classes))\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            # Calculate gradients\n",
    "            P = self._softmax(F)\n",
    "            gradients = P - y_encoded\n",
    "            \n",
    "            # Train trees for each class\n",
    "            trees_m = []\n",
    "            for k in range(n_classes):\n",
    "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "                tree.fit(X, -gradients[:, k])\n",
    "                trees_m.append(tree)\n",
    "            \n",
    "            # Update F with predictions\n",
    "            for k in range(n_classes):\n",
    "                F[:, k] += self.learning_rate * trees_m[k].predict(X)\n",
    "            \n",
    "            self.trees.append(trees_m)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Sum up predictions from all trees\n",
    "        F = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        for trees_m in self.trees:\n",
    "            for k, tree in enumerate(trees_m):\n",
    "                F[:, k] += self.learning_rate * tree.predict(X)\n",
    "        return self._softmax(F)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "# Scale the data and prepare labels\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Split the scaled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
